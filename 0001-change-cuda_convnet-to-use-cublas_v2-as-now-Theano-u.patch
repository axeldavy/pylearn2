From 9942b68902c3bd707dfcb1dc3bc89cae875ee208 Mon Sep 17 00:00:00 2001
From: Frederic <nouiz@nouiz.org>
Date: Wed, 16 Jul 2014 15:00:08 -0400
Subject: [PATCH 1/2] change cuda_convnet to use cublas_v2 as now Theano use
 this version!

---
 pylearn2/sandbox/cuda_convnet/nvmatrix.cu  | 48 +++++++++++++++++++-----------
 pylearn2/sandbox/cuda_convnet/nvmatrix.cuh | 13 ++++++--
 2 files changed, 40 insertions(+), 21 deletions(-)

diff --git a/pylearn2/sandbox/cuda_convnet/nvmatrix.cu b/pylearn2/sandbox/cuda_convnet/nvmatrix.cu
index 0dcf8b3..eb78ee7 100755
--- a/pylearn2/sandbox/cuda_convnet/nvmatrix.cu
+++ b/pylearn2/sandbox/cuda_convnet/nvmatrix.cu
@@ -31,7 +31,7 @@
 #include <set>
 #include <vector>
 #include <assert.h>
-#include <cublas.h>
+#include <cublas_v2.h>
 #include <cutil_inline.h>
 #include <stdlib.h>
 #include <stdio.h>
@@ -109,8 +109,12 @@ void NVMatrix::_init(int numRows, int numCols, int stride, bool isTrans) {
     _isTrans = isTrans;
     _devData = NULL;
     if (_numElements > 0) {
-        cublasAlloc(_numElements, sizeof(float), (void**) &_devData);
-        checkCublasError("!!!! device memory allocation error\n");
+        cudaError_t err = cudaMalloc((void**) &_devData,
+                                     _numElements * sizeof(float));
+        if (cudaSuccess != err){
+          fprintf(stderr, "!!!! device memory allocation error\n", NULL);
+            exit(EXIT_FAILURE);
+      }
     }
     _stride = stride < 0 ? getLeadingDim() : stride;
 }
@@ -171,8 +175,8 @@ NVMatrix::NVMatrix(float* devData, int numRows, int numCols, int stride, bool is
 
 NVMatrix::~NVMatrix() {
     if(_ownsData && _numElements > 0) {
-	// This line was modified by Ian Goodfellow to use device_free
-	// so that theano may keep track of device memory usage
+        // This line was modified by Ian Goodfellow to use device_free
+        // so that theano may keep track of device memory usage
         int status = device_free(_devData);
         if (status != 0) {
             fprintf(stderr, "!!!! memory free error\n");
@@ -251,10 +255,14 @@ void NVMatrix::rightMult(const NVMatrix &b, float scaleAB, NVMatrix &target) con
     if(_numRows % 64 != 0 || _numCols % 64 != 0 || b.getNumCols() % 64 != 0) {
         WARN("Matrix dimensions not divisible by 64 -- cublasSgemm performance may suffer.");
     }
-    cublasSgemm(getTransChar(), b.getTransChar(), _numRows, b.getNumCols(), _numCols,
-                scaleAB, _devData, getLeadingDim(), b.getDevData(), b.getLeadingDim(),
-                0, target.getDevData(), getNumRows());
-    checkCublasError("cublasSgemm failed");
+    cublasStatus_t err;
+    float zero = 0;
+    err = cublasSgemm(handle, getTransOp(), b.getTransOp(),
+                      _numRows, b.getNumCols(), _numCols,
+                      &scaleAB, _devData, getLeadingDim(), b.getDevData(),
+                      b.getLeadingDim(),
+                      &zero, target.getDevData(), getNumRows());
+    checkCublasError(err, "cublasSgemm failed");
 //    cudaThreadSynchronize();
 }
 
@@ -283,10 +291,13 @@ void NVMatrix::addProduct(const NVMatrix& a, const NVMatrix &b, float scaleThis,
     if(a.getNumRows() % 64 != 0 || a.getNumCols() % 64 != 0 || b.getNumCols() % 64 != 0) {
         WARN("Matrix dimensions not divisible by 64 -- cublasSgemm performance may suffer.");
     }
-    cublasSgemm(a.getTransChar(), b.getTransChar(), a.getNumRows(), b.getNumCols(), a.getNumCols(),
-                scaleAB, a.getDevData(), a.getLeadingDim(), b.getDevData(), b.getLeadingDim(),
-                scaleThis, _devData, getLeadingDim());
-    checkCublasError("cublasSgemm failed");
+    cublasStatus_t err;
+    err = cublasSgemm(handle, a.getTransOp(), b.getTransOp(),
+                      a.getNumRows(), b.getNumCols(), a.getNumCols(),
+                      &scaleAB, a.getDevData(), a.getLeadingDim(),
+                      b.getDevData(), b.getLeadingDim(),
+                      &scaleThis, _devData, getLeadingDim());
+    checkCublasError(err, "cublasSgemm failed");
 //    cudaThreadSynchronize();
 }
 
@@ -536,11 +547,12 @@ bool NVMatrix::resize(int numRows, int numCols) {
                 }
             }
             if (numRows * numCols > 0) { // allocate new memory
-                cublasStatus status = cublasAlloc(numCols * numRows, sizeof(float), (void**) &_devData);
-                if (status != CUBLAS_STATUS_SUCCESS) {
-                    fprintf(stderr, "!!!! device memory allocation error\n");
-                    exit(EXIT_FAILURE);
-                }
+              cudaError_t status = cudaMalloc((void**) &_devData,
+                                              numCols * numRows * sizeof(float));
+              if (status != cudaSuccess) {
+                fprintf(stderr, "!!!! device memory allocation error\n");
+                exit(EXIT_FAILURE);
+              }
             } else {
                 _devData = NULL;
             }
diff --git a/pylearn2/sandbox/cuda_convnet/nvmatrix.cuh b/pylearn2/sandbox/cuda_convnet/nvmatrix.cuh
index a68dba6..982951f 100755
--- a/pylearn2/sandbox/cuda_convnet/nvmatrix.cuh
+++ b/pylearn2/sandbox/cuda_convnet/nvmatrix.cuh
@@ -38,7 +38,7 @@
 
 #include <pthread.h>
 #include <map>
-#include <cublas.h>
+#include <cublas_v2.h>
 #include <cuda.h>
 #include <curand.h>
 #include <cutil_inline.h>
@@ -89,8 +89,7 @@ private:
     static std::map<int,curandState*> rndDevStates;
     static pthread_mutex_t *_rndMutex;
 
-    static void checkCublasError(const char* msg) {
-        cublasStatus status = cublasGetError();
+    static void checkCublasError(cublasStatus_t status, const char* msg) {
         if (status != CUBLAS_STATUS_SUCCESS) {
             fprintf(stderr, msg, NULL);
             exit(EXIT_FAILURE);
@@ -105,6 +104,14 @@ private:
          */
         return _isTrans ? 'n' : 't';
     }
+    cublasOperation_t getTransOp() const {
+        /*
+         * not a typo! return opposite character because a
+         * non-transposed krizhevsky matrix is in row-major order while a non-transposed
+         * cublas matrix is in column-major order.
+         */
+        return _isTrans ? CUBLAS_OP_N : CUBLAS_OP_T;
+    }
 
     void _init(int numRows, int numCols);
     void _init(int numRows, int numCols, int stride, bool isTrans);
-- 
1.7.11.7

